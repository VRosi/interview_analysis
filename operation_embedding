#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Feb 24 12:47:42 2020

@author: VictorRosi
"""

import torch
from torch.utils.tensorboard import SummaryWriter
from gensim.models.keyedvectors import KeyedVectors
import os, os.path
import numpy as np

def cos_sim(a, b):
    # Takes 2 vectors a, b and returns the cosine similarity according
    # to the definition of the dot product
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    return dot_product / (norm_a * norm_b)

test_word = ['pas','aigu']
ref_word = ['aigu']


word_list = ['grave', 'aigu', 'mÃ©dium']

model_file = '/Users/VictorRosi/Documents/GitHub/interview_analysis/embedding/frWiki_no_phrase_no_postag_500_cbow_cut10.bin'


    
# MODEL COMPUTE    
model = KeyedVectors.load_word2vec_format(model_file, binary=True, encoding='utf-8')
words_emb = model.index2word
embeddings = [model[x] for x in words_emb]
    
test_emb = []
ref_emb = []
if len(test_word) > 1: 
    emb_tmp = [model[word] for word in test_word]
    test_emb = sum(emb_tmp)
else:
    test_emb = (model[test_word[0]])

ref_emb = model[ref_word[0]]
    
    
print(cos_sim(test_emb, ref_emb))